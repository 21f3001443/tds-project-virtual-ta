## Gemini Flash Experimental Image Generation and Editing APIs

In March 2025, Google introduced native image generation and editing capabilities in the Gemini 2.0 Flash Experimental model. You can now generate and iteratively edit images via a single REST endpoint ([Experiment with Gemini 2.0 Flash native image generation](https://developers.googleblog.com/en/experiment-with-gemini-20-flash-native-image-generation/), [Generate images | Gemini API | Google AI for Developers](https://ai.google.dev/gemini-api/docs/image-generation)).

[![How to use Latest Gemini 2.0 Native Image Generation with API? (9 min)](https://i.ytimg.com/vi_webp/wgs4UYx6quY/sddefault.webp)](https://youtu.be/wgs4UYx6quY
<youtube_summary>This guide explains how to use Google Gemini 2.0's latest image generation model via API, specifically through AI Studio and Python in a Google Colab notebook. The process starts with selecting the Gemini 2.0 FL image generation model (“hot” tag) in AI Studio, uploading an input image, and providing a text prompt (e.g., “please color this image beautifully using five colors”). The output is a modified image generated by the model.

To implement this in code, you can use the "Get Code" feature in AI Studio, which provides code snippets in Python, Curl, JavaScript, and other languages. For Python, you need to install the Google Gen AI Python library, obtain an API key by creating one in AI Studio, and set this key as an environment variable or secret in your Colab or local environment.

The provided Colab notebook downloads an example image, converts it to base64, and sends it with a prompt to the Gemini 2.0 model. The model processes the request and saves the generated image locally (e.g., output.jpeg). You can customize prompts and parameters like temperature and maximum output tokens to influence creativity and output size.

Safety settings in AI Studio can be adjusted (usually turned off by default) to control content filtering. The latency is fast (~8 seconds), but Google’s rate limits may restrict production use; it's best suited for prototyping and experimentation.

You can extend this approach to other languages (JavaScript, Go, Swift) for various platforms like web, Android, and iOS. The workflow supports building SaaS applications where users upload images, and the backend uses Gemini to modify or enhance them (e.g., coloring, background removal).

The full Google Colab notebook and instructions are linked in the YouTube description. To use it, generate your own API key in AI Studio, set it in the environment, and run the code. This enables easy experimentation with Gemini 2.0 image generation via API.</youtube_summary>
) ([How to use Latest Gemini 2.0 Native Image Generation with API?](https://www.youtube.com/watch?v=wgs4UYx6quY
<youtube_summary>This text explains how to use Google's latest Gemini 2.0 FL image generation model via API, specifically through Google AI Studio and a provided Google Colab notebook using Python. The author addresses common confusion around the API usage and offers a step-by-step guide:

1. In Google AI Studio, select the Gemini 2.0 FL image generation model (tagged as "hot"), choose "image and text" input, and upload an image. You can provide prompts such as "please color this image" or customize it further.

2. Use the "Get Code" feature in AI Studio to generate code snippets in Python, Curl, JavaScript, etc. To use the API, install Google's Gen AAI Python library and obtain an API key from the Google Cloud console by creating a new API key.

3. In the Google Colab notebook (linked in the YouTube description), set the API key as an environment variable (preferably securely via secrets or environment path). Install the required Google Gen AAI Python library.

4. Prepare the input image by uploading it locally or downloading it within the notebook, saving it as image.jpeg.

5. The code converts the input image to base64, sets up the environment variable for the API key, and specifies the model name ("Gemini 2.0 flash X image generation") along with the input prompt (e.g., "please color this beautifully using five colors").

6. Parameters like temperature and max output tokens can be adjusted, especially for creative tasks or multiple chat turns. Safety settings can be toggled off if needed in AI Studio.

7. Run the code to generate and save the output image (e.g., output.jpeg) locally. The process takes around 8 seconds but may be subject to Google API rate limits, making it suitable mainly for prototyping rather than production use.

8. The author demonstrates the output image which reflects the prompt instructions, showing the model’s creative coloring ability.

9. This approach can be extended to build SaaS applications where users upload images and receive AI-generated outputs, and it supports multiple programming languages and platforms (JavaScript, Go, Kotlin, Swift, etc.).

The author encourages users to get their own API key, use the shared Google Colab notebook, and experiment with the model. The video description contains the Colab link and API key creation instructions. The Gemini 2.0 flash image generation model is currently popular and useful for various image generation and editing tasks.</youtube_summary>
))

### Simple image generation

To generate a basic image, send a POST request to the `generateContent` method:

```bash
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent?key=$GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [{ "parts": [{ "text": "A serene landscape of rolling hills at sunrise, digital art" }] }],
    "generationConfig": { "responseModalities": ["TEXT", "IMAGE"] }
  }' | jq -r '.candidates[].content.parts[] | select(.inlineData) | .inlineData.data' | base64 --decode > image.png
```

Replace `$GEMINI_API_KEY` with your key. ([Gemini API | Google AI for Developers](https://ai.google.dev/gemini-api/docs))

### Generation options

You can tweak the output with these `generationConfig` parameters:

- `responseModalities`: Modalities to return (`TEXT`, `IMAGE`).
- `temperature` (0.0–2.0): Controls randomness (default 1.0).
- `topP` (0.0–1.0): Nucleus sampling threshold.
- `topK`: Token selection cutoff.
- `maxOutputTokens`: Max tokens for text parts.
- `stopSequences`: Sequences to end generation.
- `seed`: For reproducibility.

```bash
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent?key=$GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [{ "parts": [{ "text": "A futuristic city skyline at dusk, neon lights" }] }],
    "generationConfig": {
      "responseModalities": ["TEXT", "IMAGE"],
      "temperature": 0.7,
      "topP": 0.9,
      "maxOutputTokens": 1024
    }
  }' | jq -r '.candidates[].content.parts[] | select(.inlineData) | .inlineData.data' | base64 --decode > image.png
```

[Image Generation Docs](https://ai.google.dev/gemini-api/docs/image-generation)

### Simple image editing

To edit an existing image, include it in the `contents` as `inlineData` (base64-encoded):

```bash
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent?key=$GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d '{
      "contents": [{
        "parts":[
            {"text": "Replace the background with a starry night sky"},
            {"inline_data": {"mime_type":"image/jpeg", "data": "'$(base64 -w 0 cat.jpg)'"}}
        ]
      }],
      "generationConfig": {"responseModalities": ["TEXT", "IMAGE"]}
    }' | jq -r '.candidates[].content.parts[] | select(.inlineData) | .inlineData.data' | base64 --decode > image.png
```

[Image Editing Docs](https://ai.google.dev/gemini-api/docs/image-generation)

### Editing options

Editing requests support:

- `inlineData`: Embed raw image bytes.
- `fileData`: Reference public URLs.
- All `generationConfig` options listed above.
- `safetySettings`: Per-request safety rules.
- Multi-turn edits by repeating `contents` in conversation history.

### Costs and optimization

Gemini 2.0 Flash Experimental uses token-based billing:

- **Input** (text/image/video): free tier, then $0.10 per 1M tokens.
- **Output** (text/image): free tier, then $0.40 per 1M tokens.
- **Per-image flat cost** for Pro models: ~$0.001315 /image ([Gemini Developer API Pricing | Gemini API | Google AI for Developers](https://ai.google.dev/gemini-api/docs/pricing), [Solved: Re: Outdated Gemini Pro image pricing? By tile, or...](https://www.googlecloudcommunity.com/gc/AI-ML/Outdated-Gemini-Pro-image-pricing-By-tile-or-by-image/m-p/813755)).

To optimize:

- Use smaller image sizes by setting `responseMimeType`.
- Cache or reuse prompts with `cachedContent`.
- Lower `candidateCount` or `temperature` for fewer tokens.

## OpenAI gpt-image-1 Model for Image Generation and Editing

OpenAI’s GPT Image 1 (`gpt-image-1`) is a state-of-the-art multimodal model released on April 23, 2025, for high-fidelity image creation and editing.

[![OpenAI’s New GPT Image Model API in 5 Minutes (5 min)](https://i.ytimg.com/vi_webp/k-G71JZA75A/sddefault.webp)](https://youtu.be/k-G71JZA75A
<youtube_summary>OpenAI has released the GPT Image 1 model via their API, following the popular launch of image generation in ChatGPT last month, which saw over 130 million users create 700 million images in the first week. This API allows developers to integrate high-quality, professional-grade image generation into their tools and platforms. Access is available at any developer tier but requires identity verification through the OpenAI API.

Companies like Adobe, AirTable, Figma, and Gamma already have this integrated. The API includes moderation options with auto and low filtering modes. Pricing is $5 per million tokens of input text, $10 per million tokens of image input, and $40 per million tokens of output, translating roughly to 2, 7, or 19 cents per generated image for low, medium, and high-quality square images respectively.

Users can access the model via the OpenAI playground (platform.openai.com/playground/im_images), which offers examples such as business cards and logos, with customizable aspect ratios, quality settings, and number of images. Note that usage incurs API costs even in the playground.

Setup is straightforward using the OpenAI SDK, specifying the GPT image model and prompt. A key feature is "inpainting," allowing users to upload an image and a mask to edit specific parts, useful for refining images without starting over. Masks must match format, size, and include an alpha channel.

Available aspect ratios are square, portrait, and landscape, with output images in JPEG or WEBP formats, supporting adjustable compression and transparency (transparent backgrounds). Complex prompts may take up to 2 minutes to process.

While text generation in images has improved significantly compared to earlier models like DALL·E, challenges remain in precise text placement and clarity, as well as maintaining visual consistency for recurring characters or brand elements across multiple images.

Token usage varies by quality and aspect ratio, e.g., 272 tokens for low-quality square images versus 6,240 tokens for high-quality portrait images, affecting cost and latency.

In summary, OpenAI’s GPT Image 1 API offers powerful, flexible image generation capabilities for developers with identity verification and pay-as-you-go pricing, supporting advanced features like inpainting and multiple output options.</youtube_summary>
)

### Simple image generation

Use the Image Generations endpoint:

```bash
curl 'https://api.openai.com/v1/images/generations' \
  -H 'Content-Type: application/json' \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-image-1",
    "prompt": "A whimsical illustration of a cat playing chess",
    "n": 1,
    "size": "1024x1024"
  }' > image.png
```

([Generate Image | OpenAI API - Postman](https://www.postman.com/devrel/openai/request/riub8s3/generate-image))

### Generation options

Adjust these JSON parameters:

- `model`: `gpt-image-1` (default).
- `prompt`: Text description.
- `n`: Number of images.
- `size`: `256x256`, `512x512`, or `1024x1024`.
- `response_format`: `"url"` (default) or `"b64_json"`.

```json
{
  "model": "gpt-image-1",
  "prompt": "...",
  "n": 2,
  "size": "512x512",
  "response_format": "b64_json"
}
```

### Simple image editing

Use the Edits endpoint with an image and a mask:

```bash
curl https://api.openai.com/v1/images/edits \
  -H 'Content-Type: application/json' \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-image-1",
    "image": "data:image/png;base64,<BASE64_IMAGE>",
    "mask": "data:image/png;base64,<BASE64_MASK>",
    "prompt": "Add a rainbow in the sky above the mountains",
    "n": 1,
    "size": "1024x1024"
  }'
```

([curl - What's the correct URL to test OpenAI API? - Stack Overflow](https://stackoverflow.com/questions/75041247/whats-the-correct-url-to-test-openai-api))

### Editing options

Editing requests accept:

- `image`: Original image (base64 or URL).
- `mask`: PNG mask for inpainting.
- `prompt`: Instruction for the edit.
- `n`, `size`, `response_format` as above.
- Optional `user` field for attribution.

### Costs and optimization

GPT Image 1 pricing (per 1M tokens): text input $5, image input $10, image output $40. Rough per-image costs:

- Low quality: ~$0.02
- Medium quality: ~$0.07
- High quality: ~$0.19 ([OpenAI's GPT-Image-1 API — Create Stunning Images for Your Apps!](https://medium.com/h7w/openais-gpt-image-1-api-create-stunning-images-for-your-apps-902c4f6745b1), [Usage of gpt-image-1 is priced per token, with ... - Hacker News](https://news.ycombinator.com/item?id=43787769))

To optimize:

- Choose smaller sizes (`256x256`).
- Generate fewer images (`n:1`).
- Use `response_format:"url"` to reduce payload.
- Cache frequent prompts or images.
